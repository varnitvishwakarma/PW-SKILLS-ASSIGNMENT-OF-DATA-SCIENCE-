{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6ea238a-8a8f-4f93-99c6-477e083cadf3",
   "metadata": {},
   "source": [
    "## 9 APRIL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcb6cb1-94fb-494d-af4c-b21ac34097c4",
   "metadata": {},
   "source": [
    "Q1. What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae43722-a53a-4964-8dad-9f78d60ad2ee",
   "metadata": {},
   "source": [
    "\n",
    "Bayes' theorem is a fundamental concept in probability theory and statistics. It provides a way to update our beliefs or knowledge about an event or hypothesis in light of new evidence or data. It describes how to calculate the probability of an event or hypothesis (called the posterior probability) given prior knowledge or beliefs (called the prior probability) and new observed data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cee5a2-74a7-4ecd-87d9-0742502ddb73",
   "metadata": {},
   "source": [
    "Q2. What is the formula for Bayes' theorem?\n",
    "The formula for Bayes' theorem is as follows:\n",
    "\n",
    "\\[P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\\]\n",
    "\n",
    "Where:\n",
    "- \\(P(A|B)\\) is the posterior probability of event A given B (the probability of A being true given that B is true).\n",
    "- \\(P(B|A)\\) is the conditional probability of B given A (the probability of B being true given that A is true).\n",
    "- \\(P(A)\\) is the prior probability of event A (the initial probability of A being true).\n",
    "- \\(P(B)\\) is the total probability of event B (the probability of B occurring).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6f8b55-8f54-4f96-a7ca-4a902d45fe94",
   "metadata": {},
   "source": [
    "\n",
    "Q3. How is Bayes' theorem used in practice?\n",
    "Bayes' theorem is used in various fields, including statistics, machine learning, and data analysis, for tasks such as:\n",
    "- Bayesian inference: Updating beliefs about model parameters based on observed data.\n",
    "- Spam email classification: Determining the probability that an email is spam given its content.\n",
    "- Medical diagnosis: Estimating the probability of a disease based on test results and patient information.\n",
    "- Document classification: Categorizing documents into topics or classes based on their content.\n",
    "- Natural language processing: Predicting the next word in a sentence based on previous words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ed26c9-c809-4f21-8d97-2c7c18626da7",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between Bayes' theorem and conditional probability?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Bayes' theorem is a powerful concept in probability theory and statistics that helps us make informed decisions in uncertain situations. It's named after the Reverend Thomas Bayes, an 18th-century mathematician and theologian.\n",
    "\n",
    "At its core, Bayes' theorem allows us to update our beliefs or probabilities when new evidence becomes available. It addresses questions like, \"Given what we know initially, what should we believe after considering new information?\" This concept is central to Bayesian inference.\n",
    "\n",
    "Imagine you're a doctor trying to diagnose a patient's illness. You start with prior beliefs based on your medical knowledge, representing the initial probability that the patient has a certain condition. As you perform tests and gather data, Bayes' theorem enables you to update these probabilities, taking into account the likelihood of observing the test results if the patient has the condition and the overall likelihood of those test results occurring.\n",
    "\n",
    "In essence, Bayes' theorem provides a systematic and rational way to revise your beliefs as you accumulate more information. It's not just about mathematical formulas; it's a foundational principle for reasoning under uncertainty. This theorem is widely used in various fields, from medicine and finance to machine learning, helping us make more accurate predictions and decisions by incorporating new data into our existing knowledge. It's a fundamental tool for dealing with uncertainty and improving the quality of our decisions in complex and uncertain real-world scenarios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c20e7-6b5e-4b39-b9f1-66d203c5c4a1",
   "metadata": {},
   "source": [
    "Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
    "\n",
    "\n",
    "The choice of which type of Naive Bayes classifier to use in a given problem depends on the nature of the data and the assumptions that can reasonably be made about the data. The three common types of Naive Bayes classifiers are:\n",
    "\n",
    "Gaussian Naive Bayes: This classifier assumes that the features follow a Gaussian (normal) distribution. It is suitable for continuous data where the numerical features can be modeled as normally distributed.\n",
    "\n",
    "Multinomial Naive Bayes: This classifier is appropriate for discrete data, especially when dealing with text data or count data. It assumes that features represent counts or frequencies of events, typically in the form of non-negative integers.\n",
    "\n",
    "Bernoulli Naive Bayes: This classifier is designed for binary or binary-like data, where features are either present or absent (0/1). It's commonly used in text classification tasks where each feature represents the presence or absence of a specific word in a document.\n",
    "\n",
    "The choice between these classifiers depends on the characteristics of the data and the specific requirements of the problem. It's important to consider factors such as the distribution of features, the type of data, and the underlying assumptions of each classifier when selecting the appropriate Naive Bayes variant. Additionally, it's often a good practice to try multiple variants and assess their performance empirically using cross-validation or other evaluation methods to determine which one works best for a particular problem.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477f774d-cad7-41db-986e-ae9c16c7173f",
   "metadata": {},
   "source": [
    "Q6. Assignment:\n",
    "\n",
    "\n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
    "Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\n",
    "each feature value for each class:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "\n",
    "\n",
    "\n",
    "        A 3 3 4 4 3 3 3\n",
    "        B 2 2 1 2 2 2 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\n",
    "to belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bded36fa-3a40-4bfe-b420-82d0dfaa36ed",
   "metadata": {},
   "source": [
    "To classify the new instance with features X1 = 3 and X2 = 4 using Naive Bayes, we'll calculate the posterior probabilities for each class (A and B) and choose the class with the higher posterior probability. Since you've mentioned equal prior probabilities for each class, we can skip that part of the calculation.\n",
    "\n",
    "To calculate the posterior probabilities, we'll use the Naive Bayes assumption, which assumes that the features are conditionally independent given the class. Therefore, we can calculate the probabilities for each feature separately and then multiply them together.\n",
    "\n",
    "For Class A:\n",
    "- P(X1 = 3 | A) = 4/13\n",
    "- P(X2 = 4 | A) = 3/13\n",
    "\n",
    "For Class B:\n",
    "- P(X1 = 3 | B) = 1/10\n",
    "- P(X2 = 4 | B) = 3/10\n",
    "\n",
    "Now, we'll multiply these probabilities together for each class:\n",
    "\n",
    "For Class A:\n",
    "\\[P(A | X1 = 3, X2 = 4) \\propto P(X1 = 3 | A) \\cdot P(X2 = 4 | A) \\propto (4/13) * (3/13) \\approx 0.0696\\]\n",
    "\n",
    "For Class B:\n",
    "\\[P(B | X1 = 3, X2 = 4) \\propto P(X1 = 3 | B) \\cdot P(X2 = 4 | B) \\propto (1/10) * (3/10) = 0.03\\]\n",
    "\n",
    "Since we assumed equal prior probabilities for each class, we can compare the proportional posterior probabilities directly. In this case, Class A has a higher proportional posterior probability (0.0696 vs. 0.03), so according to Naive Bayes, the new instance with features X1 = 3 and X2 = 4 would be predicted to belong to **Class A**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e7e0bf-af2c-4f29-9bfc-1494b1c77a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
