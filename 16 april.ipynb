{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7ffb489-9ae5-47b1-85f3-02bd17b8bb5d",
   "metadata": {},
   "source": [
    "### 16 APRIL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1304ab-662d-4ef7-b0f9-eb96c9dba2e9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Q1. What is boosting in machine learning?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Boosting is an ensemble machine learning technique that combines the predictions of multiple weak learners (often decision trees) to create a strong learner that can make accurate predictions. It focuses on improving the classification of difficult examples in the training data.\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "Q2. What are the advantages and limitations of using boosting techniques?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Advantages: Boosting can achieve high accuracy, handle imbalanced data, and reduce overfitting. It's versatile and can work with various base models.\n",
    "    Limitations: It can be sensitive to noisy data, may require careful hyperparameter tuning, and can be computationally expensive.\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "Q3. Explain how boosting works.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Boosting works by iteratively training weak learners on the dataset. At each iteration, it assigns higher weights to misclassified examples and lower weights to correctly classified examples. The weak learners are combined to form a strong learner, with each learner giving more weight to previously misclassified examples.\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "Q4. What are the different types of boosting algorithms?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Some common boosting algorithms include AdaBoost, Gradient Boosting, XGBoost, LightGBM, and CatBoost.\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "Q5. What are some common parameters in boosting algorithms?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Common parameters include the learning rate, number of estimators (trees), max depth of trees, subsample ratio, and min samples per leaf.\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "Q6. How do boosting algorithms combine weak learners to create a strong learner?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Boosting combines weak learners by assigning weights to their predictions and then summing or averaging the weighted predictions to make the final prediction. Each weak learner focuses on the errors made by the previous ones.\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "Q7. Explain the concept of AdaBoost algorithm and its working.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    AdaBoost (Adaptive Boosting) is a boosting algorithm. It works by training a series of weak learners (typically decision trees) sequentially. Each learner focuses on the examples that were misclassified by the previous ones. AdaBoost combines their predictions to form a strong ensemble model.\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "Q8. What is the loss function used in AdaBoost algorithm?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    AdaBoost uses an exponential loss function.\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "Q9. How does the AdaBoost algorithm update the weights of misclassified samples?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    AdaBoost increases the weights of misclassified samples so that they become more influential in subsequent iterations. This way, it gives more attention to difficulttoclassify examples.\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Increasing the number of estimators (iterations) in AdaBoost can lead to a more complex model. It may improve training accuracy but could also make the model prone to overfitting if not controlled with other hyperparameters.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
